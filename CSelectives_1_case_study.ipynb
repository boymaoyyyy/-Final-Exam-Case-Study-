{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0Kc6KC8DPwC8ASevmXc7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boymaoyyyy/-Final-Exam-Case-Study-/blob/main/CSelectives_1_case_study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Oxford-IIIT Pet + Attention U-Net (Fast Demo – Fixed & Safe)\n",
        "# =============================================================\n",
        "\n",
        "# 1. Imports and Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "rcaS-qBp0UF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2. Dataset Loading (with correct label remapping)\n",
        "# -----------------------------\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Oxford-IIIT Pet segmentation masks: 1=FG, 2=BG, 3=boundary\n",
        "# We map: 1→0 (FG), 2→1 (BG), 3→2 (ignore)\n",
        "transform_mask = transforms.Compose([\n",
        "    transforms.Resize((64, 64), interpolation=InterpolationMode.NEAREST),\n",
        "    transforms.PILToTensor(),\n",
        "    lambda x: (x.squeeze(0) - 1).long()  # Remap 1,2,3 → 0,1,2\n",
        "])\n",
        "\n",
        "train_dataset = datasets.OxfordIIITPet(\n",
        "    root=\"./data\",\n",
        "    split=\"trainval\",\n",
        "    target_types=\"segmentation\",\n",
        "    download=True,\n",
        "    transform=transform_img,\n",
        "    target_transform=transform_mask\n",
        ")\n",
        "\n",
        "val_dataset = datasets.OxfordIIITPet(\n",
        "    root=\"./data\",\n",
        "    split=\"test\",\n",
        "    target_types=\"segmentation\",\n",
        "    download=True,\n",
        "    transform=transform_img,\n",
        "    target_transform=transform_mask\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "print(\"Oxford-IIIT Pet dataset loaded!\")\n",
        "print(\"Train samples:\", len(train_dataset), \"Val samples:\", len(val_dataset))"
      ],
      "metadata": {
        "id": "eeEu08ek0WQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3. Attention U-Net (3 output classes)\n",
        "# -----------------------------\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super().__init__()\n",
        "        self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.BatchNorm2d(F_int))\n",
        "        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.BatchNorm2d(F_int))\n",
        "        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.BatchNorm2d(1), nn.Sigmoid())\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "class UNetAttention(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super().__init__()\n",
        "        def conv_block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        self.encoder1 = conv_block(3, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.encoder2 = conv_block(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.encoder3 = conv_block(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.bottleneck = conv_block(256, 512)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.att3 = AttentionBlock(256, 256, 128)\n",
        "        self.decoder3 = conv_block(512, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.att2 = AttentionBlock(128, 128, 64)\n",
        "        self.decoder2 = conv_block(256, 128)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.att1 = AttentionBlock(64, 64, 32)\n",
        "        self.decoder1 = conv_block(128, 64)\n",
        "\n",
        "        self.final = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(self.pool1(e1))\n",
        "        e3 = self.encoder3(self.pool2(e2))\n",
        "        b = self.bottleneck(self.pool3(e3))\n",
        "\n",
        "        d3 = self.up3(b)\n",
        "        e3_att = self.att3(d3, e3)\n",
        "        d3 = self.decoder3(torch.cat([d3, e3_att], dim=1))\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        e2_att = self.att2(d2, e2)\n",
        "        d2 = self.decoder2(torch.cat([d2, e2_att], dim=1))\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        e1_att = self.att1(d1, e1)\n",
        "        d1 = self.decoder1(torch.cat([d1, e1_att], dim=1))\n",
        "\n",
        "        return self.final(d1)"
      ],
      "metadata": {
        "id": "bGuNnYWe0X3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4. Model, Loss, Optimizer\n",
        "# -----------------------------\n",
        "model = UNetAttention(in_channels=3, out_channels=3).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=2)  # ignore boundary class (label=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
      ],
      "metadata": {
        "id": "i7RbCcRF0Z4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5. Training Loop (5 epochs – fast but effective)\n",
        "# -----------------------------\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        imgs = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Optional safety check (uncomment to debug label issues)\n",
        "        # assert masks.min() >= 0 and masks.max() <= 2, f\"Invalid mask: {masks.min()}, {masks.max()}\"\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)  # [B, 3, H, W]\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1} Avg Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "LcC4qphb0bRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 6. Visualize Predictions\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "imgs, masks = next(iter(val_loader))\n",
        "imgs = imgs.to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(imgs)\n",
        "    preds = outputs.argmax(dim=1).cpu()  # [B, H, W]\n",
        "\n",
        "batch_size = imgs.shape[0]\n",
        "plt.figure(figsize=(12, 3 * batch_size))\n",
        "for i in range(batch_size):\n",
        "    # Input\n",
        "    plt.subplot(batch_size, 3, i*3+1)\n",
        "    plt.imshow(imgs[i].permute(1,2,0).cpu())\n",
        "    plt.title(\"Input\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth (0=FG, 1=BG, 2=ignored)\n",
        "    plt.subplot(batch_size, 3, i*3+2)\n",
        "    plt.imshow(masks[i].cpu(), vmin=0, vmax=2, cmap='gray')\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Prediction\n",
        "    plt.subplot(batch_size, 3, i*3+3)\n",
        "    plt.imshow(preds[i], vmin=0, vmax=2, cmap='gray')\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eW3moFuV0etF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 6. Visualize Predictions\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "imgs, masks = next(iter(val_loader))\n",
        "imgs = imgs.to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(imgs)\n",
        "    preds = outputs.argmax(dim=1).cpu()  # [B, H, W]\n",
        "\n",
        "batch_size = imgs.shape[0]\n",
        "plt.figure(figsize=(12, 3 * batch_size))\n",
        "for i in range(batch_size):\n",
        "    # Input\n",
        "    plt.subplot(batch_size, 3, i*3+1)\n",
        "    plt.imshow(imgs[i].permute(1,2,0).cpu())\n",
        "    plt.title(\"Input\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth (0=FG, 1=BG, 2=ignored)\n",
        "    plt.subplot(batch_size, 3, i*3+2)\n",
        "    plt.imshow(masks[i].cpu(), vmin=0, vmax=2, cmap='gray')\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Prediction\n",
        "    plt.subplot(batch_size, 3, i*3+3)\n",
        "    plt.imshow(preds[i], vmin=0, vmax=2, cmap='gray')\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0uA48aWH0fVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXSt6KKC0gsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}